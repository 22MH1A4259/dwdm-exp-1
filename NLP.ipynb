{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgrKBDC5bK3BQXuEhhEd92",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/22MH1A4259/dwdm-exp-1/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "147FGPb_tssg",
        "outputId": "a58f1914-6e5b-4adb-8f42-db4ebf45192c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between 'alice' and 'wonderland' - CBOW :  -0.10513808\n",
            "Cosine similarity between 'alice' and 'machines' - CBOW :  -0.09575343\n",
            "Cosine similarity between 'alice' and 'wonderland' - Skip Gram :  -0.10513808\n",
            "Cosine similarity between 'alice' and 'machines' - Skip Gram :  -0.09575343\n"
          ]
        }
      ],
      "source": [
        "#Word2vec is a technique/model to produce word embedding for better word representation. It is a\n",
        "#natural language processing method that captures a large number of precise syntactic and semantic\n",
        "#word relationships. It is a shallow two-layered neural network that can detect synonymous words\n",
        "#and suggest additional words for partial sentences once it is trained.\n",
        "#Word2vec represents words in vector space representation. Words are represented in the form of\n",
        "#vectors and placement is done in such a way that similar meaning words appear together and\n",
        "#dissimilar words are located far away. This is also termed as a semantic relationship. Neural\n",
        "#networks do not understand text instead they understand only numbers. Word Embedding provides\n",
        "#a way to convert text to a numeric vector.\n",
        "\n",
        "#hai... alice this is wonderland... there are many machines...\n",
        "\n",
        "\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action = 'ignore')\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "sample = open(\"/content/word2vec.txt\")\n",
        "s = sample.read()\n",
        "\n",
        "f = s.replace(\"\\n\", \" \")\n",
        "data = []\n",
        "\n",
        "for i in sent_tokenize(f):\n",
        "    temp = []\n",
        "\n",
        "    for j in word_tokenize(i):\n",
        "        temp.append(j.lower())\n",
        "\n",
        "    data.append(temp)\n",
        "\n",
        "model1 = gensim.models.Word2Vec(data, min_count = 1,vector_size = 100, window = 5)# Print results\n",
        "\n",
        "print(\"Cosine similarity between 'alice' \" +\"and 'wonderland' - CBOW : \",model1.wv.similarity('alice', 'wonderland'))\n",
        "\n",
        "print(\"Cosine similarity between 'alice' \" +\"and 'machines' - CBOW : \",model1.wv.similarity('alice', 'machines'))\n",
        "\n",
        "model2 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100,window = 5, sg = 1)\n",
        "\n",
        "print(\"Cosine similarity between 'alice' \" +\"and 'wonderland' - Skip Gram : \",model2.wv.similarity('alice', 'wonderland'))\n",
        "\n",
        "print(\"Cosine similarity between 'alice' \" +\"and 'machines' - Skip Gram : \",model2.wv.similarity('alice', 'machines'))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wys4ODfUxnVL",
        "outputId": "815dace2-7a24-4d38-fc37-6c2ffcf51893"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}